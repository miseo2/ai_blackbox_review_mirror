import torch
import os
import pickle
import json
import torch.nn as nn
from transformers import BertTokenizer, BertModel
from ...config import Config
import logging

logger = logging.getLogger(__name__)

# VTN ëª¨ë¸ í´ë˜ìŠ¤ ì •ì˜
class VTNModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()
        self.bert = BertModel.from_pretrained("bert-base-uncased")
        self.classifier = nn.Linear(self.bert.config.hidden_size, num_classes)
        
    def forward(self, input_ids, attention_mask, labels=None):
        out = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        logits = self.classifier(out.pooler_output)
        return (nn.CrossEntropyLoss()(logits, labels), logits) if labels is not None else logits

def infer_vtn(result_data):
    """
    VTN ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì‚¬ê³  ìœ í˜•ì„ ì¶”ë¡ í•©ë‹ˆë‹¤.
    
    Args:
        result_data: ì´ì „ ë‹¨ê³„ì—ì„œ ìƒì„±ëœ ë¶„ì„ ê²°ê³¼ ë°ì´í„°
        
    Returns:
        dict: ì¶”ë¡ ëœ ì‚¬ê³  ìœ í˜• ì •ë³´
    """
    # VTN ì…ë ¥ ë°ì´í„° ì¶”ì¶œ
    video_id = result_data.get("videoId")
    vtn_pkl_path = os.path.join(Config.VTN_PKL_PATH, f'{video_id}_vtn_input.pkl')
    
    # ëª¨ë¸ ë° ë¼ë²¨ ë§µ ê²½ë¡œ
    model_dir = Config.VTN_MODEL_PATH
    label_map_path = Config.LABEL_MAP_PATH
    
    # 1. label_map.json ë¡œë“œ
    if os.path.exists(label_map_path):
        with open(label_map_path, 'r', encoding='utf-8') as f:
            raw_map = json.load(f)
        label_map = {int(k): v for k, v in raw_map.items()}
        # ì—­ë§¤í•‘ ìƒì„± (í´ë˜ìŠ¤ëª… -> ID)
        label2id = {v: int(k) for k, v in raw_map.items()}
        logger.info(f"Label map loaded: {label_map}")
    else:
        logger.error(f"Label map not found at {label_map_path}")
        return {"accident_type": "unknown"}
    
    # 2. ëª¨ë¸ ë¡œë“œ (ì»¤ìŠ¤í…€ VTNModel ì‚¬ìš©)
    model = VTNModel(len(label_map))
    ckpt_path = os.path.join(model_dir, "pytorch_model.bin")
    
    if os.path.exists(ckpt_path):
        model.load_state_dict(torch.load(ckpt_path, map_location="cpu"))
        logger.info(f"Model loaded from {ckpt_path}")
    else:
        logger.error(f"Model checkpoint not found at {ckpt_path}")
        return {"accident_type": "unknown"}
        
    model.eval()
    
    # 3. í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
    
    # 4. VTN ì…ë ¥ ë¡œë“œ
    with open(vtn_pkl_path, 'rb') as f:
        data = pickle.load(f)
    
    # 5. Bì°¨ëŸ‰ ë°©í–¥ ì¹´í…Œê³ ë¦¬ í…ì„œì—ì„œ ì¶”ì¶œ
    cat_tensor = data.get("category_tensor")
    b_idx = cat_tensor[3:6].tolist().index(1.0) if 1.0 in cat_tensor[3:6] else -1
    b_dir = ["from_left", "center", "from_right"][b_idx] if b_idx >= 0 else data.get("b_direction_text", "unknown")
    
    # ì…ë ¥ í…ìŠ¤íŠ¸ ìƒì„± (ìì—°ì–´ ë°©ì‹)
    b_text = f"B_direction is {b_dir}."
    
    bbox_str = " ".join(" ".join(map(str, b)) for b in data['bbox_sequence'])
    input_text = f"{b_text} [SEP] {bbox_str}"
    
    logger.info(f"Bì°¨ëŸ‰ ë°©í–¥: {b_dir}")
    logger.debug(f"Input text: {input_text[:200]}...")
    
    # 6. í† í¬ë‚˜ì´ì§•
    inputs = tokenizer(
        input_text,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=512
    )
    
    # 7. ì¶”ë¡ 
    with torch.no_grad():
        outputs = model(inputs['input_ids'], inputs['attention_mask'])
        logits = outputs[1] if isinstance(outputs, tuple) else outputs
        
        # 8. ë°©í–¥ì— ë”°ë¥¸ ê¸ˆì§€ëœ í´ë˜ìŠ¤ í•„í„°ë§
        forbidden = {
            "from_left":  [106, 111, 115],
            "from_right": [110, 107, 108, 109, 113, 114, 116]
        }
        
        if b_dir in forbidden:
            for forbidden_cls in forbidden[b_dir]:
                if forbidden_cls in label_map:
                    logits[0][label2id.get(forbidden_cls, -1)] = -1e9
                    logger.info(f"ğŸš« ë°©í–¥ {b_dir}ì— ë§ì§€ ì•ŠëŠ” í´ë˜ìŠ¤ í•„í„°ë§: {forbidden_cls}")
        
        pred_index = logits.argmax(dim=1).item()
    
    # 9. ì‹¤ì œ classë¡œ ë§¤í•‘
    pred_class = label_map.get(pred_index, pred_index)
    
    logger.info(f"VTN ì¶”ë¡  ì™„ë£Œ: accident_type={pred_class}")
    
    # 10. ê²°ê³¼ ë°˜í™˜
    return {
        "accident_type": pred_class
    }
